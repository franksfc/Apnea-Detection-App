/**
 * ApneaDetector - ç¡çœ å‘¼å¸æš‚åœæ£€æµ‹ç§»åŠ¨åº”ç”¨
 *
 * åŠŸèƒ½ï¼š
 * - å®æ—¶å½•éŸ³
 * - æ»‘åŠ¨çª—å£ç®—æ³•æ£€æµ‹
 * - æ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œç½®ä¿¡åº¦
 */

import React, { useState, useEffect, useRef } from 'react';
import {
  SafeAreaView,
  StyleSheet,
  Text,
  View,
  TouchableOpacity,
  ScrollView,
  Alert,
  Platform,
  Switch,
} from 'react-native';
import { check, request, PERMISSIONS, RESULTS } from 'react-native-permissions';
import RNFS from 'react-native-fs';
import KeepAwake from 'react-native-keep-awake';
import AudioProcessor from './src/AudioProcessor';
import ModelInference from './src/ModelInference';
import AudioRecorder from './src/AudioRecorder';
import AudioCaptureNative, { audioCaptureEventEmitter } from './src/AudioCaptureNative';

// æ³¨æ„ï¼šPyTorch Mobileéœ€è¦åŸç”Ÿæ¨¡å—ï¼Œè¿™é‡Œä½¿ç”¨ä¼ªä»£ç è¯´æ˜
// å®é™…éƒ¨ç½²æ—¶éœ€è¦å®‰è£…react-native-pytorch-coreæˆ–ä½¿ç”¨ONNX Runtime
// import { torch, torchvision } from 'react-native-pytorch-core';

interface DetectionResult {
  timestamp: string;
  prediction: 0 | 1;
  probNormal: string;
  probApnea: string;
}

const App = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [detectionResults, setDetectionResults] = useState<DetectionResult[]>([]);
  const [currentStatus, setCurrentStatus] = useState('ç©ºé—²');
  const [currentConfidence, setCurrentConfidence] = useState(0);
  const [detectionCount, setDetectionCount] = useState(0);
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [statusMessage, setStatusMessage] = useState('');
  const [apneaCount, setApneaCount] = useState(0); // apneaæ£€æµ‹æ¬¡æ•°
  const [enableDenoise, setEnableDenoise] = useState(false); // é™å™ªå¼€å…³
  const [enableBandpass, setEnableBandpass] = useState(false); // æ»¤æ³¢å¼€å…³
  const [keepScreenAwake, setKeepScreenAwake] = useState(false); // é˜²æ­¢æ¯å±å¼€å…³
  
  const audioRecorderPlayer = useRef(new AudioRecorder()).current;
  const audioProcessor = useRef(new AudioProcessor()).current;
  // ä½¿ç”¨è®­ç»ƒæ—¶æœ€ä½³é˜ˆå€¼0.34ï¼ˆè€Œéé»˜è®¤0.5ï¼‰
  const modelInference = useRef(new ModelInference(0.34)).current;
  
  const windowSize = 160000; // 10ç§’ @ 16kHz = 160000 samples
  const hopSize = 80000; // 5ç§’æ­¥é•¿ï¼Œç”¨äºæ»‘åŠ¨çª—å£
  const sampleRate = 16000;
  const detectionInterval = useRef<ReturnType<typeof setInterval> | null>(null);
  const recordingStartTime = useRef<number>(0);
  const durationInterval = useRef<ReturnType<typeof setInterval> | null>(null);
  const audioDataInterval = useRef<ReturnType<typeof setInterval> | null>(null); // éŸ³é¢‘æ•°æ®æ·»åŠ é—´éš”
  const waitCountRef = useRef<number>(0);
  const hasSimulatedDataRef = useRef<boolean>(false);
  const isProcessingRef = useRef<boolean>(false); // ç”¨äºåœ¨é—­åŒ…ä¸­æ£€æŸ¥å¤„ç†çŠ¶æ€
  const detectionCountRef = useRef<number>(0); // ä½¿ç”¨refå­˜å‚¨æ£€æµ‹è®¡æ•°ï¼Œé¿å…çŠ¶æ€æ›´æ–°é—®é¢˜
  const isRecordingRef = useRef<boolean>(false); // ç”¨äºåœ¨é—­åŒ…ä¸­æ£€æŸ¥å½•éŸ³çŠ¶æ€
  const isReadyForAudioDataRef = useRef<boolean>(false); // æ ‡è®°æ˜¯å¦å‡†å¤‡å¥½æ¥æ”¶éŸ³é¢‘æ•°æ®ï¼ˆç”¨äºä¸¢å¼ƒå¯åŠ¨æ—¶çš„ç¼“å†²æ•°æ®ï¼‰

  useEffect(() => {
    // æ£€æŸ¥ Hermes å¼•æ“
    try {
      // @ts-ignore - global åœ¨ React Native ç¯å¢ƒä¸­å­˜åœ¨
      if (typeof global !== 'undefined' && (global as any).HermesInternal) {
        console.log('âœ… Hermes å¼•æ“å·²å¯ç”¨');
      } else {
        console.warn('âš ï¸ Hermes å¼•æ“æœªå¯ç”¨ï¼ŒDevTools å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ');
      }
    } catch (e) {
      console.warn('âš ï¸ æ— æ³•æ£€æŸ¥ Hermes å¼•æ“çŠ¶æ€');
    }
    
    // è¯·æ±‚éº¦å…‹é£æƒé™
    requestMicrophonePermission();
    
    // åŠ è½½æ¨¡å‹
    loadModel();
    
    // è®¾ç½®éŸ³é¢‘æ•è·äº‹ä»¶ç›‘å¬
    let audioDataListener: any = null;
    if (audioCaptureEventEmitter) {
      audioDataListener = audioCaptureEventEmitter.addListener(
        'onAudioData',
        (event: { audioData: number[]; sampleCount: number }) => {
          // åªæœ‰åœ¨å‡†å¤‡å¥½æ¥æ”¶æ•°æ®æ—¶æ‰æ·»åŠ ï¼ˆç”¨äºä¸¢å¼ƒå¯åŠ¨æ—¶çš„ç¼“å†²æ•°æ®ï¼‰
          if (isReadyForAudioDataRef.current && event.audioData && event.audioData.length > 0) {
            audioProcessor.addAudioData(event.audioData);
          }
        }
      );
    }
    
    return () => {
      // æ¸…ç†èµ„æº
      if (detectionInterval.current) {
        clearInterval(detectionInterval.current);
      }
      if (audioDataListener) {
        audioDataListener.remove();
      }
      // ç¡®ä¿åœ¨ç»„ä»¶å¸è½½æ—¶ç¦ç”¨é˜²æ­¢æ¯å±
      KeepAwake.deactivate();
      stopRecording();
    };
  }, []);

  const requestMicrophonePermission = async () => {
    try {
      const permission = Platform.OS === 'android' 
        ? PERMISSIONS.ANDROID.RECORD_AUDIO 
        : PERMISSIONS.IOS.MICROPHONE;

      const result = await check(permission);
      
      if (result === RESULTS.GRANTED) {
        console.log('éº¦å…‹é£æƒé™å·²æˆäºˆ');
      } else {
        const requestResult = await request(permission);
        if (requestResult !== RESULTS.GRANTED) {
          Alert.alert('æƒé™è¢«æ‹’ç»', 'éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½ä½¿ç”¨æ­¤åº”ç”¨');
        }
      }
    } catch (err) {
      console.warn('æƒé™è¯·æ±‚é”™è¯¯:', err);
    }
  };

  const loadModel = async () => {
    try {
      console.log('æ­£åœ¨åŠ è½½æ¨¡å‹...');
      setStatusMessage('æ­£åœ¨åŠ è½½æ¨¡å‹æ–‡ä»¶...');
      
      // å°è¯•åŠ è½½çœŸå®æ¨¡å‹æ–‡ä»¶
      let modelPath: string | undefined;
      let preprocessorPath: string | undefined;
      
      if (Platform.OS === 'android') {
        // Android: assetsæ–‡ä»¶éœ€è¦ä½¿ç”¨ asset:// åè®®
        // åŸç”Ÿæ¨¡å—ä¼šè‡ªåŠ¨ä»assetsç›®å½•åŠ è½½æ–‡ä»¶
        try {
          // å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
          const possiblePaths = [
            // Android assets ç›®å½•ï¼ˆåŸç”Ÿæ¨¡å—ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            'asset://apnea_model.pt',
            // ä»assetså¤åˆ¶åˆ°å¯è®¿é—®ç›®å½•åçš„è·¯å¾„
            `${RNFS.DocumentDirectoryPath}/apnea_model.pt`,
            // Bundleè·¯å¾„
            `${RNFS.MainBundlePath}/apnea_model.pt`,
          ];
          
          // é¦–å…ˆå°è¯•assetè·¯å¾„ï¼ˆæ¨èï¼ŒåŸç”Ÿæ¨¡å—ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
          modelPath = 'asset://apnea_model.pt';
          preprocessorPath = 'asset://audio_preprocessor.pt';
          console.log('ä½¿ç”¨assetè·¯å¾„åŠ è½½æ¨¡å‹ï¼ˆåŸç”Ÿæ¨¡å—ä¼šè‡ªåŠ¨å¤„ç†ï¼‰:', modelPath);
          
          // å¯é€‰ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨å…¶ä»–ä½ç½®
          // for (const path of possiblePaths.slice(1)) {
          //   try {
          //     const exists = await RNFS.exists(path);
          //     if (exists) {
          //       modelPath = path;
          //       const ppPath = path.replace('apnea_model.pt', 'audio_preprocessor.pt');
          //       if (await RNFS.exists(ppPath)) {
          //         preprocessorPath = ppPath;
          //       }
          //       console.log('æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶:', modelPath);
          //       break;
          //     }
          //   } catch (e) {
          //     continue;
          //   }
          // }
        } catch (e) {
          console.log('æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ—¶å‡ºé”™:', e);
        }
      } else {
        // iOS: ä½¿ç”¨Bundleè·¯å¾„
        try {
          const possiblePaths = [
            `${RNFS.MainBundlePath}/apnea_model.pt`,
            `${RNFS.DocumentDirectoryPath}/apnea_model.pt`,
          ];
          
          for (const path of possiblePaths) {
            const exists = await RNFS.exists(path);
            if (exists) {
              modelPath = path;
              const ppPath = path.replace('apnea_model.pt', 'audio_preprocessor.pt');
              if (await RNFS.exists(ppPath)) {
                preprocessorPath = ppPath;
              }
              console.log('æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶:', modelPath);
              break;
            }
          }
        } catch (e) {
          console.log('æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ—¶å‡ºé”™:', e);
        }
      }
      
          // åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæ‰¾åˆ°äº†æ–‡ä»¶ï¼‰
          if (modelPath) {
            console.log('å°è¯•åŠ è½½æ¨¡å‹æ–‡ä»¶:', modelPath);
            await modelInference.loadModel(modelPath, preprocessorPath || null);
            
            // è®¾ç½®Temperature Scalingï¼ˆæé«˜ç½®ä¿¡åº¦ï¼‰
            try {
              await modelInference.setTemperature(0.7); // æ¨èå€¼ï¼š0.7-0.9ï¼Œ0.7æ›´è‡ªä¿¡
              console.log('Temperature Scalingå·²è®¾ç½®: 0.7');
            } catch (e) {
              console.warn('è®¾ç½®Temperatureå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼:', e);
            }
            
            // ç¡®ä¿ä½¿ç”¨æœ€ä½³é˜ˆå€¼ï¼ˆè®­ç»ƒæ—¶æœ€ä½³é˜ˆå€¼çº¦ä¸º0.34ï¼‰
            modelInference.setThreshold(0.34);
            console.log(`åˆ†ç±»é˜ˆå€¼å·²è®¾ç½®ä¸º: ${modelInference.getThreshold().toFixed(3)} (è®­ç»ƒæ—¶æœ€ä½³é˜ˆå€¼)`);
            
            setStatusMessage('æ¨¡å‹åŠ è½½æˆåŠŸ');
            console.log('çœŸå®æ¨¡å‹åŠ è½½æˆåŠŸ');
          } else {
            // ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            await modelInference.loadModel(); // ä¸ä¼ è·¯å¾„ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            setStatusMessage('ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼‰');
            console.log('ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼‰');
          }
      
      setCurrentStatus('å°±ç»ª');
    } catch (error) {
      console.error('æ¨¡å‹åŠ è½½å¤±è´¥:', error);
      // å¦‚æœåŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
      try {
        await modelInference.loadModel();
        setStatusMessage('æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå·²åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼');
        setCurrentStatus('å°±ç»ªï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰');
      } catch (e) {
        Alert.alert('é”™è¯¯', 'æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶');
        setCurrentStatus('é”™è¯¯');
      }
    }
  };

  const startRecording = async () => {
    try {
      // åŒæ­¥éŸ³é¢‘å¤„ç†è®¾ç½®
      audioProcessor.setDenoiseEnabled(enableDenoise);
      audioProcessor.setBandpassEnabled(enableBandpass);
      
      // æ¸…ç©ºç¼“å†²åŒº
      audioProcessor.clear();
      detectionCountRef.current = 0;
      setDetectionCount(0);
      setApneaCount(0); // é‡ç½®apneaè®¡æ•°
      setRecordingDuration(0);
      hasSimulatedDataRef.current = false; // é‡ç½®æ¨¡æ‹Ÿæ•°æ®æ ‡è®°
      isReadyForAudioDataRef.current = false; // æ ‡è®°ä¸ºæœªå‡†å¤‡å¥½ï¼Œç”¨äºä¸¢å¼ƒå¯åŠ¨æ—¶çš„ç¼“å†²æ•°æ®
      setCurrentStatus('æ­£åœ¨å¯åŠ¨å½•éŸ³...');
      setStatusMessage('æ­£åœ¨åˆå§‹åŒ–å½•éŸ³è®¾å¤‡...');
      
      // å¼€å§‹å½•éŸ³
      const audioSet = {
        AudioEncoderAndroid: 'AAC',
        AudioSourceAndroid: 'MIC',
        AVModeIOSOption: 'measurement',
        AVEncoderAudioQualityKeyIOS: 'high',
        AVNumberOfChannelsKeyIOS: 1,
        AVFormatIDKeyIOS: 'aac',
      };
      
      const uri = await audioRecorderPlayer.startRecorder(
        `${RNFS.CachesDirectoryPath}/apnea_recording.aac`,
        audioSet
      );
      
      audioRecorderPlayer.addRecordBackListener((e) => {
        // å®æ—¶è·å–éŸ³é¢‘æ•°æ®ï¼ˆç›®å‰åŸç”Ÿæ¨¡å—ä¸æä¾›å®æ—¶æ•°æ®æµï¼‰
        // è¿™é‡Œä¸»è¦ç”¨äºæ˜¾ç¤ºå½•éŸ³è¿›åº¦
        console.log('å½•éŸ³ä¸­...', e.currentPosition);
      });
      
      setIsRecording(true);
      isRecordingRef.current = true; // æ›´æ–°ref
      recordingStartTime.current = Date.now();
      setCurrentStatus('ğŸ”´ å½•éŸ³ä¸­');
      setStatusMessage('æ­£åœ¨æ”¶é›†éŸ³é¢‘æ•°æ®ï¼Œç­‰å¾…é¦–æ¬¡æ£€æµ‹...');
      
      // å¯åŠ¨å®æ—¶éŸ³é¢‘æ•è·ï¼ˆä½¿ç”¨AudioRecordï¼‰
      // å…ˆç¡®ä¿ä¹‹å‰çš„èµ„æºå·²å®Œå…¨é‡Šæ”¾
      if (AudioCaptureNative) {
        try {
          // å…ˆå°è¯•åœæ­¢ï¼Œç¡®ä¿èµ„æºå·²é‡Šæ”¾ï¼ˆå³ä½¿ä¹‹å‰å¯èƒ½å·²ç»åœæ­¢ï¼‰
          try {
            await AudioCaptureNative.stopCapture();
            // ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿èµ„æºå®Œå…¨é‡Šæ”¾
            await new Promise<void>(resolve => setTimeout(() => resolve(), 200));
          } catch (e) {
            // å¦‚æœåœæ­¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æœªåœ¨æ•è·ï¼‰ï¼Œå¿½ç•¥é”™è¯¯
            console.log('æ¸…ç†ä¹‹å‰çš„éŸ³é¢‘æ•è·èµ„æºï¼ˆå¯èƒ½å·²åœæ­¢ï¼‰');
          }
          
          // ç°åœ¨å¯åŠ¨æ–°çš„æ•è·
          await AudioCaptureNative.startCapture();
          console.log('å®æ—¶éŸ³é¢‘æ•è·å·²å¯åŠ¨');
          
          // é‡è¦ï¼šç­‰å¾…ç³»ç»Ÿç¨³å®šï¼Œå¹¶ä¸¢å¼ƒå¯åŠ¨æ—¶çš„ç¼“å†²æ•°æ®
          // AudioRecord åœ¨å¯åŠ¨æ—¶å¯èƒ½ä¼šè¯»å–åˆ°ä¸€äº›ç³»ç»Ÿç¼“å†²çš„æ—§æ•°æ®
          // æˆ‘ä»¬éœ€è¦ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œè®©ç³»ç»Ÿç¨³å®šï¼Œç„¶åæ¸…ç©ºç¼“å†²åŒº
          console.log('ç­‰å¾…éŸ³é¢‘ç³»ç»Ÿç¨³å®šï¼Œä¸¢å¼ƒå¯èƒ½çš„ç¼“å†²æ•°æ®...');
          await new Promise<void>(resolve => setTimeout(() => resolve(), 500));
          
          // å†æ¬¡æ¸…ç©ºç¼“å†²åŒºï¼Œç¡®ä¿ä¸ä½¿ç”¨å¯åŠ¨æ—¶çš„ç¼“å†²æ•°æ®
          audioProcessor.clear();
          
          // ç°åœ¨æ ‡è®°ä¸ºå‡†å¤‡å¥½æ¥æ”¶éŸ³é¢‘æ•°æ®ï¼ˆä¹‹å‰çš„ç¼“å†²æ•°æ®å·²è¢«ä¸¢å¼ƒï¼‰
          isReadyForAudioDataRef.current = true;
          console.log('éŸ³é¢‘ç¼“å†²åŒºå·²æ¸…ç©ºï¼Œå¼€å§‹ä½¿ç”¨çœŸå®å½•éŸ³æ•°æ®');
        } catch (error: any) {
          console.error('å¯åŠ¨å®æ—¶éŸ³é¢‘æ•è·å¤±è´¥:', error);
          // å¦‚æœå®æ—¶æ•è·å¤±è´¥ï¼Œé™çº§åˆ°æ¨¡æ‹Ÿæ¨¡å¼
          console.warn('é™çº§åˆ°æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®æ¨¡å¼');
        }
      } else {
        console.warn('AudioCaptureNativeä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®');
        // æ¨¡æ‹Ÿæ¨¡å¼ä¸‹ç«‹å³æ ‡è®°ä¸ºå‡†å¤‡å¥½æ¥æ”¶æ•°æ®
        isReadyForAudioDataRef.current = true;
      }
      
      // å¯åŠ¨å½•éŸ³æ—¶é•¿è®¡æ—¶å™¨
      durationInterval.current = setInterval(() => {
        const duration = Math.floor((Date.now() - recordingStartTime.current) / 1000);
        setRecordingDuration(duration);
      }, 1000);
      
      // å¯åŠ¨æ»‘åŠ¨çª—å£æ£€æµ‹å¾ªç¯
      startSlidingWindowDetection();
      
    } catch (error) {
      console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error);
      Alert.alert('é”™è¯¯', 'æ— æ³•å¼€å§‹å½•éŸ³');
      setIsRecording(false);
      setCurrentStatus('å½•éŸ³å¤±è´¥');
      setStatusMessage('');
    }
  };

  const stopRecording = async () => {
    try {
      // å…ˆåœæ­¢å½•éŸ³çŠ¶æ€
      isRecordingRef.current = false;
      setIsRecording(false);
      
      // æ¸…ç†æ‰€æœ‰é—´éš”
      if (detectionInterval.current) {
        clearInterval(detectionInterval.current);
        detectionInterval.current = null;
      }
      
      if (audioDataInterval.current) {
        clearInterval(audioDataInterval.current);
        audioDataInterval.current = null;
      }
      
      if (durationInterval.current) {
        clearInterval(durationInterval.current);
        durationInterval.current = null;
      }
      
      // åœæ­¢å®æ—¶éŸ³é¢‘æ•è·
      if (AudioCaptureNative) {
        try {
          await AudioCaptureNative.stopCapture();
          console.log('å®æ—¶éŸ³é¢‘æ•è·å·²åœæ­¢');
          // ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿èµ„æºå®Œå…¨é‡Šæ”¾
          await new Promise<void>(resolve => setTimeout(() => resolve(), 100));
        } catch (error: any) {
          console.error('åœæ­¢å®æ—¶éŸ³é¢‘æ•è·å¤±è´¥:', error);
          // å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­ï¼Œå› ä¸ºå¯èƒ½å·²ç»åœæ­¢äº†
        }
      }
      
      // é‡ç½®éŸ³é¢‘æ•°æ®æ¥æ”¶æ ‡å¿—
      isReadyForAudioDataRef.current = false;
      
      // åœæ­¢å½•éŸ³
      const result = await audioRecorderPlayer.stopRecorder();
      audioRecorderPlayer.removeRecordBackListener();
      
      setCurrentStatus('å·²åœæ­¢');
      setStatusMessage(`å½•éŸ³å·²åœæ­¢ï¼Œå…±è¿›è¡Œäº† ${detectionCountRef.current} æ¬¡æ£€æµ‹`);
      setRecordingDuration(0);
      audioProcessor.clear();
    } catch (error) {
      console.error('åœæ­¢å½•éŸ³å¤±è´¥:', error);
      setStatusMessage('åœæ­¢å½•éŸ³æ—¶å‡ºé”™');
    }
  };

  const startSlidingWindowDetection = () => {
    // é‡ç½®ç­‰å¾…è®¡æ•°å’Œæ¨¡æ‹Ÿæ•°æ®æ ‡è®°
    waitCountRef.current = 0;
    hasSimulatedDataRef.current = false;
    
    // æ³¨æ„ï¼šå¦‚æœAudioCaptureNativeå¯ç”¨ï¼ŒéŸ³é¢‘æ•°æ®ä¼šé€šè¿‡äº‹ä»¶ç›‘å¬å™¨è‡ªåŠ¨æ·»åŠ 
    // è¿™é‡Œä¸å†éœ€è¦æ¨¡æ‹Ÿæ•°æ®
    // å¦‚æœAudioCaptureNativeä¸å¯ç”¨ï¼Œä¿ç•™æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºåå¤‡æ–¹æ¡ˆ
    if (!AudioCaptureNative) {
      console.warn('AudioCaptureNativeä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®');
      audioDataInterval.current = setInterval(() => {
        if (!isRecordingRef.current) {
          if (audioDataInterval.current) {
            clearInterval(audioDataInterval.current);
            audioDataInterval.current = null;
          }
          return;
        }
        
        // æ¨¡æ‹Ÿæ¯0.5ç§’çš„éŸ³é¢‘æ•°æ®ï¼ˆ0.5ç§’ @ 16kHz = 8000 samplesï¼‰
        const chunkSize = sampleRate * 0.5; // 0.5ç§’çš„æ•°æ®
        const simulatedChunk = new Array(chunkSize).fill(0).map(() => Math.random() * 0.1 - 0.05);
        audioProcessor.addAudioData(simulatedChunk);
      }, 500); // æ¯0.5ç§’æ·»åŠ ä¸€æ¬¡æ•°æ®
    }
    
    // æ»‘åŠ¨çª—å£æ£€æµ‹å¾ªç¯ï¼ˆæ¯5ç§’æ£€æµ‹ä¸€æ¬¡ï¼‰
    detectionInterval.current = setInterval(async () => {
      // æ£€æŸ¥æ˜¯å¦è¿˜åœ¨å½•éŸ³ï¼ˆä½¿ç”¨refï¼‰
      if (!isRecordingRef.current) {
        if (detectionInterval.current) {
          clearInterval(detectionInterval.current);
          detectionInterval.current = null;
        }
        if (audioDataInterval.current) {
          clearInterval(audioDataInterval.current);
          audioDataInterval.current = null;
        }
        return;
      }
      
      // å¦‚æœæ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æµ‹
      if (isProcessingRef.current) {
        return;
      }
      
      try {
        // æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼ˆ10ç§’ï¼‰
        if (audioProcessor.hasEnoughData()) {
          // è·å–æ»‘åŠ¨çª—å£ï¼ˆæœ€è¿‘10ç§’çš„æ•°æ®ï¼Œåº”ç”¨é¢„å¤„ç†ï¼‰
          const audioWindow = audioProcessor.getNextWindow(true);
          
          if (audioWindow && audioWindow.length === windowSize) {
            // è¿›è¡Œæ£€æµ‹
            await processAudioWindow(audioWindow);
          } else {
            console.warn('éŸ³é¢‘çª—å£æ•°æ®é•¿åº¦ä¸æ­£ç¡®:', audioWindow?.length);
          }
        } else {
          // æ•°æ®ä¸è¶³ï¼Œæ˜¾ç¤ºç­‰å¾…ä¿¡æ¯
          waitCountRef.current++;
          const bufferedDuration = audioProcessor.getBufferedDuration();
          
          if (bufferedDuration < 10) {
            setStatusMessage(`æ­£åœ¨æ”¶é›†éŸ³é¢‘æ•°æ®... (éœ€è¦10ç§’ï¼Œå·²æ”¶é›† ${bufferedDuration.toFixed(1)}ç§’)`);
          } else {
            setStatusMessage('ç­‰å¾…éŸ³é¢‘æ•°æ®è¾¾åˆ°10ç§’...');
          }
        }
      } catch (error) {
        console.error('æ£€æµ‹å¾ªç¯é”™è¯¯:', error);
        setStatusMessage('æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯');
        // ç¡®ä¿é”™è¯¯åé‡ç½®å¤„ç†çŠ¶æ€
        isProcessingRef.current = false;
        setIsProcessing(false);
      }
    }, 5000); // æ¯5ç§’æ£€æµ‹ä¸€æ¬¡
  };

  const processAudioWindow = async (audioData: number[]) => {
    // åŒé‡æ£€æŸ¥ï¼Œé¿å…é‡å¤å¤„ç†
    if (isProcessingRef.current) {
      console.log('å·²åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æµ‹');
      return;
    }
    
    if (!modelInference.isModelLoaded()) {
      console.warn('æ¨¡å‹æœªåŠ è½½');
      setStatusMessage('æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œæ£€æµ‹');
      return;
    }

    // éªŒè¯éŸ³é¢‘æ•°æ®
    if (!audioData || audioData.length !== windowSize) {
      console.warn('éŸ³é¢‘æ•°æ®é•¿åº¦ä¸æ­£ç¡®:', audioData?.length, 'æœŸæœ›:', windowSize);
      return;
    }

    // è®¾ç½®å¤„ç†çŠ¶æ€ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰æ£€æŸ¥ä¹‹åï¼‰
    isProcessingRef.current = true;
    setIsProcessing(true);
    
    // å¢åŠ æ£€æµ‹è®¡æ•°
    detectionCountRef.current += 1;
    const currentCount = detectionCountRef.current;
    setDetectionCount(currentCount);
    
    setStatusMessage(`æ­£åœ¨åˆ†æéŸ³é¢‘æ•°æ®... (ç¬¬ ${currentCount} æ¬¡æ£€æµ‹)`);
    
    try {
      console.log(`å¼€å§‹æ‰§è¡Œæ¨¡å‹æ¨ç† #${currentCount}ï¼ŒéŸ³é¢‘æ•°æ®å¤§å°:`, audioData.length);
      
      // æ‰§è¡Œæ¨¡å‹æ¨ç†ï¼ˆå¼‚æ­¥æ“ä½œï¼‰
      const result = await modelInference.predict(audioData);
      
      if (!result) {
        throw new Error('æ¨¡å‹æ¨ç†è¿”å›ç©ºç»“æœ');
      }
      
      if (typeof result.prediction !== 'number' || 
          typeof result.probNormal !== 'number' || 
          typeof result.probApnea !== 'number') {
        throw new Error('æ¨¡å‹æ¨ç†ç»“æœæ ¼å¼ä¸æ­£ç¡®');
      }
      
      console.log(`æ¨¡å‹æ¨ç†æˆåŠŸ #${currentCount}:`, result);
      
      // æ›´æ–°çŠ¶æ€
      const timestamp = new Date().toLocaleTimeString();
      const detectionResult: DetectionResult = {
        timestamp,
        prediction: result.prediction,
        probNormal: (result.probNormal * 100).toFixed(1),
        probApnea: (result.probApnea * 100).toFixed(1),
      };
      
      // æ ¹æ®æ£€æµ‹ç»“æœæ›´æ–°çŠ¶æ€
      if (result.prediction === 1) {
        setCurrentStatus('âš ï¸ æ£€æµ‹åˆ°å‘¼å¸æš‚åœ');
        setStatusMessage(`âš ï¸ è­¦å‘Šï¼šç¬¬ ${currentCount} æ¬¡æ£€æµ‹å‘ç°å‘¼å¸æš‚åœï¼`);
        // æ›´æ–°apneaè®¡æ•°
        setApneaCount(prev => prev + 1);
      } else {
        setCurrentStatus('âœ… æ­£å¸¸å‘¼å¸');
        setStatusMessage(`âœ… ç¬¬ ${currentCount} æ¬¡æ£€æµ‹ï¼šæœªæ£€æµ‹åˆ°å‘¼å¸æš‚åœï¼Œå‘¼å¸æ­£å¸¸`);
      }
      
      setCurrentConfidence(Math.max(result.probNormal, result.probApnea) * 100);
      
      // æ— è®ºç»“æœå¦‚ä½•ï¼Œéƒ½æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ï¼ˆåŒ…æ‹¬æ­£å¸¸ç»“æœï¼‰
      setDetectionResults(prev => {
        const newResults = [detectionResult, ...prev].slice(0, 20);
        return newResults;
      });
      
      console.log(`æ£€æµ‹å®Œæˆ #${currentCount}:`, detectionResult);
      
    } catch (error) {
      console.error('å¤„ç†éŸ³é¢‘å¤±è´¥:', error);
      console.error('é”™è¯¯è¯¦æƒ…:', error instanceof Error ? error.message : String(error));
      console.error('é”™è¯¯å †æ ˆ:', error instanceof Error ? error.stack : 'æ— å †æ ˆä¿¡æ¯');
      
      setCurrentStatus('å¤„ç†é”™è¯¯');
      const errorMsg = error instanceof Error ? error.message : String(error);
      setStatusMessage(`æ£€æµ‹å¤±è´¥: ${errorMsg}`);
      
      // æ˜¾ç¤ºé”™è¯¯æç¤ºï¼ˆä½†ä¸é˜»å¡åç»­æ£€æµ‹ï¼‰
      Alert.alert(
        'æ£€æµ‹é”™è¯¯',
        `ç¬¬ ${currentCount} æ¬¡æ£€æµ‹å¤±è´¥: ${errorMsg}\n\nè¯·æ£€æŸ¥:\n1. æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½\n2. éŸ³é¢‘æ•°æ®æ˜¯å¦æœ‰æ•ˆ\n3. æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯`,
        [{ text: 'ç¡®å®š' }]
      );
    } finally {
      // ç¡®ä¿åœ¨å¤„ç†å®Œæˆåé‡ç½®çŠ¶æ€
      isProcessingRef.current = false;
      setIsProcessing(false);
      
      // 1ç§’åæ¢å¤çŠ¶æ€æç¤º
      setTimeout(() => {
        if (isRecordingRef.current && !isProcessingRef.current) {
          setStatusMessage('ç­‰å¾…ä¸‹ä¸€æ¬¡æ£€æµ‹...');
        }
      }, 1000);
    }
  };

  const clearResults = () => {
    setDetectionResults([]);
    setCurrentStatus('å°±ç»ª');
    setCurrentConfidence(0);
    detectionCountRef.current = 0;
    setDetectionCount(0);
    setApneaCount(0);
    setStatusMessage('');
  };

  // å¤„ç†é™å™ªå¼€å…³å˜åŒ–
  const handleDenoiseToggle = (value: boolean) => {
    setEnableDenoise(value);
    audioProcessor.setDenoiseEnabled(value);
    console.log(`é™å™ªå·²${value ? 'å¯ç”¨' : 'ç¦ç”¨'}`);
  };

  // å¤„ç†æ»¤æ³¢å¼€å…³å˜åŒ–
  const handleBandpassToggle = (value: boolean) => {
    setEnableBandpass(value);
    audioProcessor.setBandpassEnabled(value);
    console.log(`å¸¦é€šæ»¤æ³¢å·²${value ? 'å¯ç”¨' : 'ç¦ç”¨'}`);
  };

  // å¤„ç†é˜²æ­¢æ¯å±å¼€å…³å˜åŒ–
  const handleKeepAwakeToggle = (value: boolean) => {
    setKeepScreenAwake(value);
    if (value) {
      KeepAwake.activate();
      console.log('å·²å¯ç”¨é˜²æ­¢æ¯å±');
    } else {
      KeepAwake.deactivate();
      console.log('å·²ç¦ç”¨é˜²æ­¢æ¯å±');
    }
  };

  // è®¡ç®—apneaæ¯”ä¾‹å’ŒçŠ¶æ€
  const getApneaRatio = (): { ratio: number; status: string; color: string; bgColor: string } => {
    if (detectionCount === 0) {
      return { ratio: 0, status: 'æš‚æ— æ•°æ®', color: '#666', bgColor: '#f5f5f5' };
    }
    
    const ratio = (apneaCount / detectionCount) * 100;
    
    if (ratio < 33) {
      return { ratio, status: 'ä¸æ€€ç–‘', color: '#4CAF50', bgColor: '#e8f5e9' };
    } else if (ratio <= 66) {
      return { ratio, status: 'æ€€ç–‘', color: '#FF9800', bgColor: '#fff3e0' };
    } else {
      return { ratio, status: 'é«˜åº¦æ€€ç–‘', color: '#F44336', bgColor: '#ffebee' };
    }
  };

  return (
    <SafeAreaView style={styles.container}>
      {/* é˜²æ­¢æ¯å±ç»„ä»¶ */}
      {keepScreenAwake && <KeepAwake />}
      <ScrollView style={styles.scrollView}>
        <View style={styles.header}>
          <Text style={styles.title}>ç¡çœ å‘¼å¸æš‚åœæ£€æµ‹</Text>
          <Text style={styles.subtitle}>å®æ—¶ç›‘æµ‹æ‚¨çš„å‘¼å¸çŠ¶æ€</Text>
        </View>

        {/* çŠ¶æ€æ˜¾ç¤º */}
        <View style={styles.statusContainer}>
          <Text style={styles.statusLabel}>å½“å‰çŠ¶æ€</Text>
          <Text style={[styles.statusText, currentStatus.includes('âš ï¸') && styles.warningText]}>
            {currentStatus}
          </Text>
          {isRecording && (
            <Text style={styles.recordingDurationText}>
              å½•éŸ³æ—¶é•¿: {Math.floor(recordingDuration / 60)}:{(recordingDuration % 60).toString().padStart(2, '0')}
            </Text>
          )}
          {detectionCount > 0 && (
            <Text style={styles.detectionCountText}>
              æ£€æµ‹æ¬¡æ•°: {detectionCount}
            </Text>
          )}
          {currentConfidence > 0 && (
            <Text style={styles.confidenceText}>
              ç½®ä¿¡åº¦: {currentConfidence.toFixed(1)}%
            </Text>
          )}
          {statusMessage ? (
            <View style={styles.statusMessageContainer}>
              <Text style={styles.statusMessageText}>{statusMessage}</Text>
            </View>
          ) : null}
          {isProcessing && (
            <View style={styles.processingIndicator}>
              <Text style={styles.processingText}>â³ æ­£åœ¨å¤„ç†ä¸­...</Text>
            </View>
          )}
        </View>

        {/* Apneaæ¯”ä¾‹æ˜¾ç¤º */}
        {detectionCount > 0 && (
          <View style={[
            styles.apneaRatioContainer, 
            { 
              backgroundColor: getApneaRatio().bgColor,
              borderColor: getApneaRatio().color,
            }
          ]}>
            <Text style={styles.apneaRatioLabel}>æœ¬æ¬¡å½•éŸ³ Apnea æ¯”ä¾‹</Text>
            <View style={styles.apneaRatioContent}>
              <Text style={[styles.apneaRatioValue, { color: getApneaRatio().color }]}>
                {getApneaRatio().ratio.toFixed(1)}%
              </Text>
              <Text style={[styles.apneaRatioStatus, { color: getApneaRatio().color }]}>
                {getApneaRatio().status}
              </Text>
            </View>
            <Text style={styles.apneaRatioDetail}>
              ({apneaCount} / {detectionCount} æ¬¡æ£€æµ‹ä¸º Apnea)
            </Text>
          </View>
        )}

        {/* éŸ³é¢‘å¤„ç†è®¾ç½® */}
        <View style={styles.settingsContainer}>
          <Text style={styles.settingsTitle}>éŸ³é¢‘å¤„ç†è®¾ç½®</Text>
          
          <View style={styles.settingItem}>
            <View style={styles.settingLabelContainer}>
              <Text style={styles.settingLabel}>é™å™ªå¤„ç†</Text>
              <Text style={styles.settingDescription}>
                å‡å°‘èƒŒæ™¯å™ªå£°ï¼Œæé«˜æ£€æµ‹å‡†ç¡®ç‡
              </Text>
            </View>
            <Switch
              value={enableDenoise}
              onValueChange={handleDenoiseToggle}
              disabled={isRecording}
              trackColor={{ false: '#767577', true: '#4A90E2' }}
              thumbColor={enableDenoise ? '#fff' : '#f4f3f4'}
            />
          </View>

          <View style={styles.settingItem}>
            <View style={styles.settingLabelContainer}>
              <Text style={styles.settingLabel}>å¸¦é€šæ»¤æ³¢</Text>
              <Text style={styles.settingDescription}>
                ä¿ç•™100-2000Hzé¢‘æ®µï¼Œè¿‡æ»¤æ— å…³é¢‘ç‡
              </Text>
            </View>
            <Switch
              value={enableBandpass}
              onValueChange={handleBandpassToggle}
              disabled={isRecording}
              trackColor={{ false: '#767577', true: '#4A90E2' }}
              thumbColor={enableBandpass ? '#fff' : '#f4f3f4'}
            />
          </View>

          <View style={styles.settingItem}>
            <View style={styles.settingLabelContainer}>
              <Text style={styles.settingLabel}>é˜²æ­¢æ¯å±</Text>
              <Text style={styles.settingDescription}>
                åº”ç”¨åœ¨å‰å°æ—¶ä¿æŒå±å¹•å¸¸äº®
              </Text>
            </View>
            <Switch
              value={keepScreenAwake}
              onValueChange={handleKeepAwakeToggle}
              trackColor={{ false: '#767577', true: '#4A90E2' }}
              thumbColor={keepScreenAwake ? '#fff' : '#f4f3f4'}
            />
          </View>
        </View>

        {/* æ§åˆ¶æŒ‰é’® */}
        <View style={styles.controlContainer}>
          <TouchableOpacity
            style={[styles.button, isRecording && styles.buttonStop]}
            onPress={isRecording ? stopRecording : startRecording}
            disabled={isProcessing}
          >
            <Text style={styles.buttonText}>
              {isRecording ? 'åœæ­¢æ£€æµ‹' : 'å¼€å§‹æ£€æµ‹'}
            </Text>
          </TouchableOpacity>

          {detectionResults.length > 0 && (
            <TouchableOpacity
              style={[styles.button, styles.buttonClear]}
              onPress={clearResults}
            >
              <Text style={styles.buttonText}>æ¸…ç©ºè®°å½•</Text>
            </TouchableOpacity>
          )}
        </View>

        {/* æ£€æµ‹ç»“æœåˆ—è¡¨ */}
        {detectionResults.length > 0 && (
          <View style={styles.resultsContainer}>
            <Text style={styles.resultsTitle}>æ£€æµ‹è®°å½•</Text>
            {detectionResults.map((result, index) => (
              <View
                key={index}
                style={[
                  styles.resultItem,
                  result.prediction === 1 && styles.resultItemWarning,
                ]}
              >
                <View style={styles.resultHeader}>
                  <Text style={styles.resultTime}>{result.timestamp}</Text>
                  <Text
                    style={[
                      styles.resultStatus,
                      result.prediction === 1 && styles.resultStatusWarning,
                    ]}
                  >
                    {result.prediction === 1 ? 'âš ï¸ å‘¼å¸æš‚åœ' : 'âœ… æ­£å¸¸'}
                  </Text>
                </View>
                <View style={styles.resultDetails}>
                  <Text style={styles.resultDetail}>
                    æ­£å¸¸: {result.probNormal}%
                  </Text>
                  <Text style={styles.resultDetail}>
                    å‘¼å¸æš‚åœ: {result.probApnea}%
                  </Text>
                </View>
              </View>
            ))}
          </View>
        )}

        {/* ä½¿ç”¨è¯´æ˜ */}
        <View style={styles.infoContainer}>
          <Text style={styles.infoTitle}>ä½¿ç”¨è¯´æ˜</Text>
          <Text style={styles.infoText}>
            1. ç‚¹å‡»"å¼€å§‹æ£€æµ‹"æŒ‰é’®å¼€å§‹å½•éŸ³{'\n'}
            2. åº”ç”¨å°†ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•å®æ—¶åˆ†æéŸ³é¢‘{'\n'}
            3. æ¯5ç§’è¿›è¡Œä¸€æ¬¡æ£€æµ‹ï¼Œæ˜¾ç¤ºæœ€è¿‘çš„ç»“æœ{'\n'}
            4. æ£€æµ‹åˆ°å‘¼å¸æš‚åœæ—¶ä¼šæ˜¾ç¤ºè­¦å‘Š{'\n'}
            5. ç‚¹å‡»"åœæ­¢æ£€æµ‹"ç»“æŸç›‘æ§
          </Text>
    </View>
      </ScrollView>
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#f5f5f5',
  },
  scrollView: {
    flex: 1,
  },
  header: {
    backgroundColor: '#4A90E2',
    padding: 20,
    alignItems: 'center',
  },
  title: {
    fontSize: 24,
    fontWeight: 'bold',
    color: '#fff',
    marginBottom: 5,
  },
  subtitle: {
    fontSize: 14,
    color: '#fff',
    opacity: 0.9,
  },
  statusContainer: {
    backgroundColor: '#fff',
    margin: 15,
    padding: 20,
    borderRadius: 10,
    alignItems: 'center',
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
  },
  statusLabel: {
    fontSize: 14,
    color: '#666',
    marginBottom: 10,
  },
  statusText: {
    fontSize: 24,
    fontWeight: 'bold',
    color: '#4CAF50',
    marginBottom: 5,
  },
  warningText: {
    color: '#F44336',
  },
  recordingDurationText: {
    fontSize: 16,
    color: '#4A90E2',
    marginTop: 8,
    fontWeight: '600',
  },
  detectionCountText: {
    fontSize: 14,
    color: '#666',
    marginTop: 5,
  },
  confidenceText: {
    fontSize: 16,
    color: '#666',
    marginTop: 5,
  },
  statusMessageContainer: {
    marginTop: 15,
    padding: 12,
    backgroundColor: '#f0f7ff',
    borderRadius: 8,
    borderLeftWidth: 4,
    borderLeftColor: '#4A90E2',
    width: '100%',
  },
  statusMessageText: {
    fontSize: 14,
    color: '#333',
    lineHeight: 20,
  },
  processingIndicator: {
    marginTop: 10,
    padding: 8,
    backgroundColor: '#fff3cd',
    borderRadius: 6,
  },
  processingText: {
    fontSize: 14,
    color: '#856404',
    fontWeight: '500',
  },
  controlContainer: {
    flexDirection: 'row',
    justifyContent: 'center',
    marginHorizontal: 15,
    marginBottom: 15,
    gap: 10,
  },
  button: {
    flex: 1,
    backgroundColor: '#4A90E2',
    padding: 15,
    borderRadius: 8,
    alignItems: 'center',
  },
  buttonStop: {
    backgroundColor: '#F44336',
  },
  buttonClear: {
    backgroundColor: '#9E9E9E',
    flex: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 16,
    fontWeight: 'bold',
  },
  resultsContainer: {
    margin: 15,
    backgroundColor: '#fff',
    borderRadius: 10,
    padding: 15,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
  },
  resultsTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    marginBottom: 15,
    color: '#333',
  },
  resultItem: {
    padding: 12,
    borderLeftWidth: 4,
    borderLeftColor: '#4CAF50',
    backgroundColor: '#f9f9f9',
    borderRadius: 5,
    marginBottom: 10,
  },
  resultItemWarning: {
    borderLeftColor: '#F44336',
    backgroundColor: '#fff5f5',
  },
  resultHeader: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    marginBottom: 5,
  },
  resultTime: {
    fontSize: 12,
    color: '#666',
  },
  resultStatus: {
    fontSize: 14,
    fontWeight: 'bold',
    color: '#4CAF50',
  },
  resultStatusWarning: {
    color: '#F44336',
  },
  resultDetails: {
    flexDirection: 'row',
    justifyContent: 'space-around',
    marginTop: 5,
  },
  resultDetail: {
    fontSize: 12,
    color: '#666',
  },
  infoContainer: {
    margin: 15,
    backgroundColor: '#fff',
    borderRadius: 10,
    padding: 15,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
  },
  infoTitle: {
    fontSize: 16,
    fontWeight: 'bold',
    marginBottom: 10,
    color: '#333',
  },
  infoText: {
    fontSize: 14,
    color: '#666',
    lineHeight: 22,
  },
  settingsContainer: {
    margin: 15,
    backgroundColor: '#fff',
    borderRadius: 10,
    padding: 15,
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
  },
  settingsTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    marginBottom: 15,
    color: '#333',
  },
  settingItem: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    paddingVertical: 12,
    borderBottomWidth: 1,
    borderBottomColor: '#f0f0f0',
  },
  settingLabelContainer: {
    flex: 1,
    marginRight: 15,
  },
  settingLabel: {
    fontSize: 16,
    fontWeight: '600',
    color: '#333',
    marginBottom: 4,
  },
  settingDescription: {
    fontSize: 12,
    color: '#666',
    lineHeight: 16,
  },
  apneaRatioContainer: {
    margin: 15,
    padding: 20,
    borderRadius: 10,
    alignItems: 'center',
    shadowColor: '#000',
    shadowOffset: { width: 0, height: 2 },
    shadowOpacity: 0.1,
    shadowRadius: 4,
    elevation: 3,
    borderWidth: 2,
    borderColor: 'transparent',
  },
  apneaRatioLabel: {
    fontSize: 14,
    color: '#666',
    marginBottom: 10,
    fontWeight: '500',
  },
  apneaRatioContent: {
    flexDirection: 'row',
    alignItems: 'baseline',
    justifyContent: 'center',
    marginBottom: 8,
  },
  apneaRatioValue: {
    fontSize: 32,
    fontWeight: 'bold',
    marginRight: 10,
  },
  apneaRatioStatus: {
    fontSize: 20,
    fontWeight: '600',
  },
  apneaRatioDetail: {
    fontSize: 12,
    color: '#666',
    marginTop: 5,
  },
});

export default App;
